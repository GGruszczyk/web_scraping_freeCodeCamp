# -*- coding: utf-8 -*-
"""Copia de web_scraping_freeCodeCamp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hrdzi2JqttrbYtEh2FqidNPu7d68C7uj

<center>
<img src="https://d33wubrfki0l68.cloudfront.net/774b60156d8f103170dc66f3ad10310941114653/da262/img/fcc_secondary_large.svg" width="600" height="auto"/>

# **Web scraping con Python**

## *Material complementario del curso dictado por [Gustavo Juantorena](https://github.com/GEJ1) para **freeCodecamp** en español.*

### **Link a la web de práctica: https://scrapepark.org/courses/spanish/**

<center>

### **Importante**: Los cambios que hagan en este cuaderno de Colab no se guardarán, lo ideal sería que hagan una copia del mismo en sus respectivas cuentas de Google Drive de la siguiente manera:

### *Archivo > Guardar una copia en drive*

# **Hoja de ruta**

## 1. Pedidos HTTP con **Requests**
## 2. Uso basico de **APIs**
## 3. Web Scraping con **Beautiful Soup**

# **Pedidos HTTP con requests**
"""

import requests
import json

# Hacemos un pedido a la página de wikipedia
URL = 'https://es.wikipedia.org/'

# Guardamos el objeto que nos devuelve
respuesta = requests.get(URL)

# print(f'Tipo de Objeto: {type(respuesta)} \n')
# print(f'Código de estado: {respuesta.status_code} \n')
print(f'Data: {respuesta.text} \n')

"""## **Headers**

Una serie de datos que acompañan al pedido. Para saber más: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers


El objeto `Response` de `requests` tiene los siguientes elementos principales:

* `.text`
* `.content`
* `.json()`
* `.status_code`
"""

URL = 'https://scrapepark.org/courses/spanish/'

headers = {
    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'
}
respuesta = requests.get(URL, headers=headers)

respuesta.text

"""Veamoslo en la práctica utilizando la siguiente web: http://httpbin.org/headers (útil para testear pedidos HTTP).

"""

URL = 'http://httpbin.org/headers'
resp = requests.get(URL)

print('Respuesta sin headers:')
print(resp.text)

print('Respuesta con headers:')
nuestros_headers = {
    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'
    }
resp_con_headers = requests.get(URL, headers = nuestros_headers)
print(resp_con_headers.text)

"""# **Uso basico de APIs**

### Uso de API de manera directa

[Sunset and sunrise times API](https://sunrise-sunset.org/api)

**Sirve para obtener la hora del amanecer y el ocaso de un determinado día**

*Parámetros:*


*  **lat** (float): Latitud en grados decimales(Obligatorio)
*  **lng** (float): Longitud en grados decimales (obligatorio)
*  **date** (string): Fecha en formato AAAA-MM-DD (opcional, por defecto usa el día actual)

*Estructura de la query:*

`https://api.sunrise-sunset.org/json?`

`lat=36.7201600`

`&`

`lng=-4.4203400`

`&`

`date=2021-07-26`
"""

# Definimos los parametros de nuestra query
latitud = -34.6
longitud = -58.4
fecha = '1816-07-09' # AAAA-MM-DD

# Hacemos el pedido y guardamos la respuesta en una nueva variable
respuesta_sunset = requests.get(f'https://api.sunrise-sunset.org/json?lat={latitud}&lng={longitud}&date={fecha}')

type(respuesta_sunset)

# Para des-serializar el objeto (que era tipo 'HTTPResponse') y cargarlo como json
datos_sunset = respuesta_sunset.json()
print(datos_sunset)

type(datos_sunset)
datos_sunset.keys()

# Evaluamos el status del pedido
sunset_status = datos_sunset['status']
print(f'Status: {sunset_status}')

datos_sunset['results']['sunset']

# Podemos ver su contenido ya que es son diccionarios anidados:
sunset = datos_sunset['results']['sunset']
print(f'El {fecha} el sol se ocultó a las {sunset} (UTC)')

# tambien podriamos iterar sobre sus claves
print("Iterando data_sunset['results']:")
for elemento in datos_sunset['results']:
  print(elemento)

"""### **Uso de API por medio de una librería: Wikipedia**

Wikipedia-API es un *wrapper* de Python fácil de usar para la API de Wikipedia. Admite la extracción de textos, secciones, enlaces, categorías, traducciones, etc.

Repositorio: https://github.com/martin-majlis/Wikipedia-API

Documentación: https://wikipedia-api.readthedocs.io/en/latest/README.html





"""

# Instalamos el paquete porque no viene con Colab
!pip3 install --force-reinstall -v  "wikipedia-api==0.5.8"

# Ahora si podemos importarlo
import wikipediaapi

# Chequear versión
print(wikipediaapi.__version__)

# Instanciamos la clase wikipediaapi y utilizamos el metodo Wikipedia con el parametro de idioma
IDIOMA = 'es'
wiki_wiki = wikipediaapi.Wikipedia(IDIOMA)

# Usamos el metodo page para y hacemos un pedido con una palabra clave
PALABRA_CLAVE = 'programación'
wikipedia_programacion = wiki_wiki.page(PALABRA_CLAVE)

print(f'wikipedia_programacion es un objeto de tipo: \n \n{type(wikipedia_programacion)}')

# Resumen
print(wikipedia_programacion.title)
print(' ')
print(wikipedia_programacion.summary)

# Url completa
print(wikipedia_programacion.fullurl)

"""# **BeautifulSoup**
Documentación oficial: https://beautiful-soup-4.readthedocs.io/en/latest/

## **Generalidades**

Vamos a practicar con https://scrapepark.org/spanish/
"""

from bs4 import BeautifulSoup
import requests

# Versiones
import bs4 # Solo para el chequeo
print("Versión de BeautifulSoup:",bs4.__version__)
print("Versión de requests:", requests.__version__)

# En caso de no tener la versión que se usa en este curso
!pip3 install beautifulsoup4==4.11.2
!pip3 install requests==2.27.1

# Empezamos el scraping

# 1. Obtener el HTML
URL_BASE = 'https://scrapepark.org/courses/spanish/'
pedido_obtenido = requests.get(URL_BASE)
html_obtenido = pedido_obtenido.text

# 2. "Parsear" ese HTML
soup = BeautifulSoup(html_obtenido, "html.parser")

type(soup)

"""## **El método `find()`**

Nos permite quedarnos con la información asociada a una etiqueta de HTML
"""

primer_h2 = soup.find('h2')
print(primer_h2)

# Solo el texto
print(primer_h2.text)

# equivalente a:
# print(soup.h2.text)

"""## **El método `find_all()`**

Busca **TODOS** los elementos de la página con esa etiqueta y devuelve una "lista" que los contiene (en realidad devuelve un objeto de la clase *bs4.element.ResultSet*).
"""

h2_todos = soup.find_all('h2')
print(h2_todos)

# ARGUMENTOS
# Si usamos el parametro limit = 1, emulamos al metodo find
h2_uno_solo = soup.find_all('h2',limit=1)
print(h2_uno_solo)

# Podemos iterar sobre el objeto
for seccion in h2_todos:
  print(seccion.text)

# get_text() para más funcionalidades
for seccion in h2_todos:
  print(seccion.get_text(strip=True))

"""## **Utilizando atributos de las etiquetas**


"""

# Clase
divs = soup.find_all('div', class_ = "heading-container heading-center")

for div in divs:
  print(div)
  print(" ")

# Todas las etiquetas que tengan el atributo "src"
src_todos = soup.find_all(src=True)

for elemento in src_todos:
  if elemento['src'].endswith(".jpg"):
    print(elemento)

# src_todos

#@title Ejercicio: Bajar todas las imagenes!

url_imagenes = []

for i, imagen in enumerate(src_todos):

  if imagen['src'].endswith('png'):

    print(imagen['src'])
    r = requests.get(f"https://scrapepark.org/courses/spanish/{imagen['src']}")

    with open(f'imagen_{i}.png', 'wb') as f:
      f.write(r.content)

"""## **Tablas**"""

soup.find_all('iframe')[0]['src']

# Información de tablas

URL_BASE = 'https://scrapepark.org/courses/spanish'
URL_TABLA = soup.find_all('iframe')[0]['src']

request_tabla = requests.get(f'{URL_BASE}/{URL_TABLA}')

html_tabla = request_tabla.text
soup_tabla = BeautifulSoup(html_tabla, "html.parser")
soup_tabla.find('table')

productos_faltantes = soup_tabla.find_all(['th', 'td'], attrs={'style':'color: red;'})
productos_faltantes = [talle.text for talle in productos_faltantes]

print(productos_faltantes)

divs = soup.find_all('div', class_='detail-box')
productos = []
precios = []

for div in divs:
  if (div.h6 is not None) and ('Patineta' in div.h5.text):
    producto = div.h5.get_text(strip=True)
    precio = div.h6.get_text(strip=True).replace('$', '')
    # Se puede agregar filtros
    print(f'producto: {producto:<17} | precio: {precio}')
    productos.append(producto)
    precios.append(precio)

precios

productos

"""## **Cambios que dependen de la URL**"""

URL_BASE = "https://scrapepark.org/courses/spanish/contact"

for i in range(1,3):
  URL_FINAL = f"{URL_BASE}{i}"
  print(URL_FINAL)
  r = requests.get(URL_FINAL)
  soup = BeautifulSoup(r.text, "html.parser")
  print(soup.h5.text)

"""## **Datos que no sabemos en que parte de la página se encuentran**"""

# Expresiones regulares
import re

# 1. Obtener el HTML
URL_BASE = 'https://scrapepark.org/courses/spanish'
pedido_obtenido = requests.get(URL_BASE)
html_obtenido = pedido_obtenido.text

# 2. "Parsear" ese HTML
soup = BeautifulSoup(html_obtenido, "html.parser")

telefonos = soup.find_all(string=re.compile("\d+-\d+-\d+"))
telefonos

"""## **Moviéndonos por el árbol**

Para saber más: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree
"""

copyrights = soup.find_all(string=re.compile("©"))
copyrights[0]

primer_copyright = copyrights[0]
primer_copyright.parent

# # Otro ejemplo con elementos al mismo nivel
menu = soup.find(string=re.compile("MENÚ"))
# menu.parent
menu.parent.find_next_siblings()

"""## **Comentario sobre excepciones**
https://docs.python.org/es/3/tutorial/errors.html
"""

strings_a_buscar = ["MENÚ", "©", "carpincho", "Patineta"]

for string in strings_a_buscar:
  try:
    resultado = soup.find(string=re.compile(string))
    print(resultado.text)
  except AttributeError:
    print(f"El string '{string}' no fue encontrado")

"""## **Almacenamiento de los datos**"""

productos.insert(0, "productos")
precios.insert(0, "precios")
# datos = dict(zip(productos, precios))

datos = dict(zip(productos, precios))

datos.items()

import csv

with open('datos.csv','w') as f:
    w = csv.writer(f)
    w.writerows(datos.items())

"""**BONUS!**
Algunos ejercicios para seguir practicando:

1. Las patinetas que salgan menos que $68
2. Las patinetas que en su nombre tengan un numero mayor a 3
3. Traer cualquier texto de la pagina que tenga la palabra descuento u oferta.
5. Generar un archivo .csv con dos columnas: Una conteniendo el nombre del cliente y otra su testimonio.
"""



"""Las patinetas que salgan menos que $68"""

divs = soup.find_all('div', class_='detail-box')
productos = []
precios = []

for div in divs:
    if (div.h6 is not None) and ('Patineta' in div.h5.text):
        producto = div.h5.get_text(strip=True)
        precio = div.h6.get_text(strip=True).replace('$', '')
        if int(precio) < 68:
            print(f'Producto: {producto} | Precio: {precio}')
            productos.append(producto)
            precios.append(precio)

"""Las patinetas cuyo nombre contiene un número mayor a 3

"""

divs = soup.find_all('div', class_='detail-box')
productos = []

for div in divs:
    if (div.h6 is not None) and ('Patineta' in div.h5.text):
        producto = div.h5.get_text(strip=True)
        if any(char.isdigit() and int(char) > 3 for char in producto):
            print(f'Producto: {producto}')
            productos.append(producto)

"""Texto de la página con las palabras "descuento" u "oferta"
"""

textos = soup.find_all(string=re.compile(r'descuento|oferta', flags=re.IGNORECASE))

for i, texto in enumerate(textos, start=1):
    print(f"{i}. {texto}")

"""Generar un archivo .csv con dos columnas, nombre del cliente y testimonio"""

# Supongamos que tenemos una lista de clientes y sus testimonios
clientes = ["Cliente1", "Cliente2", "Cliente3"]
testimonios = ["Excelente servicio", "Muy satisfecho con el producto", "Recomiendo esta empresa"]

# Crear un diccionario con los datos
datos = {"Nombre del Cliente": clientes, "Testimonio": testimonios}

import csv

# Escribir los datos en un archivo CSV
with open('testimonios.csv', 'w', newline='') as csvfile:
    fieldnames = ['Nombre del Cliente', 'Testimonio']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for i in range(len(clientes)):
        writer.writerow({'Nombre del Cliente': clientes[i], 'Testimonio': testimonios[i]})

print("Archivo CSV generado exitosamente.")

